{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: opencv-python in c:\\programdata\\anaconda3\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: mediapipe in c:\\programdata\\anaconda3\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: sklearn in c:\\programdata\\anaconda3\\lib\\site-packages (0.0.post5)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: seaborn in c:\\programdata\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.54.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.11)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (4.7.0.72)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.39.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\programdata\\anaconda3\\lib\\site-packages (from seaborn) (2.0.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas>=0.25->seaborn) (2022.7)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.10.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.29.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.3.5)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow opencv-python mediapipe sklearn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Coordinates of left hand, right hand, arms and nose from Mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 127\u001b[0m\n\u001b[0;32m    123\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m left_hand_landmarks, right_hand_landmarks, arm_landmarks, nose_landmarks\n\u001b[1;32m--> 127\u001b[0m left_hand_landmarks, right_hand_landmarks, arm_landmarks, nose_landmarks \u001b[38;5;241m=\u001b[39m \u001b[43mshow_landmarks_on_webcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 89\u001b[0m, in \u001b[0;36mshow_landmarks_on_webcam\u001b[1;34m()\u001b[0m\n\u001b[0;32m     86\u001b[0m width, height \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m480\u001b[39m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Open webcam\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m cap \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoCapture\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m cap\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;241m3\u001b[39m, width)\n\u001b[0;32m     91\u001b[0m cap\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;241m4\u001b[39m, height)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def extract_landmarks(frame, left_hand_landmarks, right_hand_landmarks, arm_landmarks, nose_landmarks):\n",
    "    image_height, image_width, _ = frame.shape\n",
    "\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "\n",
    "    with mp_holistic.Holistic(static_image_mode=False, model_complexity=2, min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        # Convert the image to RGB\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the image\n",
    "        results = holistic.process(image_rgb)\n",
    "\n",
    "        # Left hand landmarks\n",
    "        if results.left_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "            for landmark in results.left_hand_landmarks.landmark:\n",
    "                x = int(landmark.x * image_width)\n",
    "                y = int(landmark.y * image_height)\n",
    "                left_hand_landmarks.append((x, y))\n",
    "                cv2.putText(frame, f\"Left Hand: ({x}, {y})\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)\n",
    "\n",
    "        # Right hand landmarks\n",
    "        if results.right_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "            for landmark in results.right_hand_landmarks.landmark:\n",
    "                x = int(landmark.x * image_width)\n",
    "                y = int(landmark.y * image_height)\n",
    "                right_hand_landmarks.append((x, y))\n",
    "                cv2.putText(frame, f\"Right Hand: ({x}, {y})\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 0, 255), 1)\n",
    "\n",
    "        # Arm landmarks\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "            arm_landmarks_indices = [mp.solutions.holistic.PoseLandmark.LEFT_WRIST,\n",
    "                                     mp.solutions.holistic.PoseLandmark.RIGHT_WRIST,\n",
    "                                     mp.solutions.holistic.PoseLandmark.LEFT_ELBOW,\n",
    "                                     mp.solutions.holistic.PoseLandmark.RIGHT_ELBOW,\n",
    "                                     mp.solutions.holistic.PoseLandmark.LEFT_SHOULDER,\n",
    "                                     mp.solutions.holistic.PoseLandmark.RIGHT_SHOULDER]\n",
    "            for landmark_idx in arm_landmarks_indices:\n",
    "                landmark = results.pose_landmarks.landmark[landmark_idx]\n",
    "                x = int(landmark.x * image_width)\n",
    "                y = int(landmark.y * image_height)\n",
    "                arm_landmarks.append((x, y))\n",
    "                cv2.putText(frame, f\"Arm: ({x}, {y})\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)\n",
    "\n",
    "        # Nose landmark\n",
    "        if results.pose_landmarks:\n",
    "            nose_landmark = results.pose_landmarks.landmark[mp.solutions.holistic.PoseLandmark.NOSE]\n",
    "            x = int(nose_landmark.x * image_width)\n",
    "            y = int(nose_landmark.y * image_height)\n",
    "            nose_landmarks.append((x, y))\n",
    "            cv2.putText(frame, f\"Nose: ({x}, {y})\", (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 1)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def process_landmarks(left_hand_landmarks, right_hand_landmarks, arm_landmarks, nose_landmarks):\n",
    "    # Process the left-hand, right-hand, arm, and nose landmarks\n",
    "    # Here, we simply print the coordinates\n",
    "    print(\"Left Hand Landmarks:\")\n",
    "    for idx, landmark in enumerate(left_hand_landmarks):\n",
    "        print(f\"Landmark {idx}: {landmark}\")\n",
    "    print()\n",
    "    print(\"Right Hand Landmarks:\")\n",
    "    for idx, landmark in enumerate(right_hand_landmarks):\n",
    "        print(f\"Landmark {idx}: {landmark}\")\n",
    "    print()\n",
    "    print(\"Arm Landmarks:\")\n",
    "    for idx, landmark in enumerate(arm_landmarks):\n",
    "        print(f\"Landmark {idx}: {landmark}\")\n",
    "    print()\n",
    "    print(\"Nose Landmarks:\")\n",
    "    for idx, landmark in enumerate(nose_landmarks):\n",
    "        print(f\"Landmark {idx}: {landmark}\")\n",
    "    print()\n",
    "\n",
    "def show_landmarks_on_webcam():\n",
    "    mp_holistic = mp.solutions.holistic.Holistic(static_image_mode=False, model_complexity=2, min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "    \n",
    "    # Webcam parameters\n",
    "    width, height = 640, 480\n",
    "\n",
    "    # Open webcam\n",
    "    cap = cv2.VideoCapture(1)\n",
    "    cap.set(3, width)\n",
    "    cap.set(4, height)\n",
    "\n",
    "    left_hand_landmarks = []\n",
    "    right_hand_landmarks = []\n",
    "    arm_landmarks = []\n",
    "    nose_landmarks = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        # Rescale the frame by a factor of 0.5\n",
    "        #frame = cv2.resize(frame, None, fx=0.5, fy=0.5)\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = extract_landmarks(frame, left_hand_landmarks, right_hand_landmarks, arm_landmarks, nose_landmarks)\n",
    "\n",
    "        process_landmarks(left_hand_landmarks, right_hand_landmarks, arm_landmarks, nose_landmarks)\n",
    "\n",
    "        image_height, image_width, _ = frame.shape\n",
    "        origin = (image_width // 2, image_height // 2)\n",
    "        x_axis = (image_width, image_height // 2)\n",
    "        y_axis = (image_width // 2, image_height)\n",
    "        cv2.putText(frame, f\"Origin: {origin}\", (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "        cv2.putText(frame, f\"X-axis: {x_axis}\", (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "        cv2.putText(frame, f\"Y-axis: {y_axis}\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
    "\n",
    "        cv2.imshow('Landmarks', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return left_hand_landmarks, right_hand_landmarks, arm_landmarks, nose_landmarks\n",
    "\n",
    "left_hand_landmarks, right_hand_landmarks, arm_landmarks, nose_landmarks = show_landmarks_on_webcam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcap\u001b[49m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m      2\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cap' is not defined"
     ]
    }
   ],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Collect Keypoint Values for Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataSet collection with data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for 'Lock Person' - Repetition 1\n",
      "Collecting data for 'Lock Person' - Repetition 2\n",
      "Collecting data for 'Lock Person' - Repetition 3\n",
      "Collecting data for 'Lock Person' - Repetition 4\n",
      "Collecting data for 'Lock Person' - Repetition 5\n",
      "Collecting data for 'Lock Person' - Repetition 6\n",
      "Collecting data for 'Lock Person' - Repetition 7\n",
      "Collecting data for 'Lock Person' - Repetition 8\n",
      "Collecting data for 'Lock Person' - Repetition 9\n",
      "Collecting data for 'Lock Person' - Repetition 10\n",
      "Collecting data for 'Lock Person' - Repetition 11\n",
      "Collecting data for 'Lock Person' - Repetition 12\n",
      "Collecting data for 'Lock Person' - Repetition 13\n",
      "Collecting data for 'Lock Person' - Repetition 14\n",
      "Collecting data for 'Lock Person' - Repetition 15\n",
      "Collecting data for 'Lock Person' - Repetition 16\n",
      "Collecting data for 'Lock Person' - Repetition 17\n",
      "Collecting data for 'Lock Person' - Repetition 18\n",
      "Collecting data for 'Lock Person' - Repetition 19\n",
      "Collecting data for 'Lock Person' - Repetition 20\n",
      "Collecting data for 'Lock Person' - Repetition 21\n",
      "Collecting data for 'Lock Person' - Repetition 22\n",
      "Collecting data for 'Lock Person' - Repetition 23\n",
      "Collecting data for 'Lock Person' - Repetition 24\n",
      "Collecting data for 'Lock Person' - Repetition 25\n",
      "Collecting data for 'Lock Person' - Repetition 26\n",
      "Collecting data for 'Lock Person' - Repetition 27\n",
      "Collecting data for 'Lock Person' - Repetition 28\n",
      "Collecting data for 'Lock Person' - Repetition 29\n",
      "Collecting data for 'Lock Person' - Repetition 30\n",
      "Collecting data for 'Unlock Person' - Repetition 1\n",
      "Collecting data for 'Unlock Person' - Repetition 2\n",
      "Collecting data for 'Unlock Person' - Repetition 3\n",
      "Collecting data for 'Unlock Person' - Repetition 4\n",
      "Collecting data for 'Unlock Person' - Repetition 5\n",
      "Collecting data for 'Unlock Person' - Repetition 6\n",
      "Collecting data for 'Unlock Person' - Repetition 7\n",
      "Collecting data for 'Unlock Person' - Repetition 8\n",
      "Collecting data for 'Unlock Person' - Repetition 9\n",
      "Collecting data for 'Unlock Person' - Repetition 10\n",
      "Collecting data for 'Unlock Person' - Repetition 11\n",
      "Collecting data for 'Unlock Person' - Repetition 12\n",
      "Collecting data for 'Unlock Person' - Repetition 13\n",
      "Collecting data for 'Unlock Person' - Repetition 14\n",
      "Collecting data for 'Unlock Person' - Repetition 15\n",
      "Collecting data for 'Unlock Person' - Repetition 16\n",
      "Collecting data for 'Unlock Person' - Repetition 17\n",
      "Collecting data for 'Unlock Person' - Repetition 18\n",
      "Collecting data for 'Unlock Person' - Repetition 19\n",
      "Data collection paused. Press 'p' to resume.\n",
      "Resuming data collection.\n",
      "Collecting data for 'Unlock Person' - Repetition 20\n",
      "Data collection paused. Press 'p' to resume.\n",
      "Resuming data collection.\n",
      "Collecting data for 'Unlock Person' - Repetition 21\n",
      "Collecting data for 'Unlock Person' - Repetition 22\n",
      "Collecting data for 'Unlock Person' - Repetition 23\n",
      "Collecting data for 'Unlock Person' - Repetition 24\n",
      "Data collection paused. Press 'p' to resume.\n",
      "Resuming data collection.\n",
      "Collecting data for 'Unlock Person' - Repetition 25\n",
      "Collecting data for 'Unlock Person' - Repetition 26\n",
      "Collecting data for 'Unlock Person' - Repetition 27\n",
      "Collecting data for 'Unlock Person' - Repetition 28\n",
      "Collecting data for 'Unlock Person' - Repetition 29\n",
      "Collecting data for 'Unlock Person' - Repetition 30\n",
      "Collecting data for 'Go to Base' - Repetition 1\n",
      "Collecting data for 'Go to Base' - Repetition 2\n",
      "Collecting data for 'Go to Base' - Repetition 3\n",
      "Collecting data for 'Go to Base' - Repetition 4\n",
      "Collecting data for 'Go to Base' - Repetition 5\n",
      "Collecting data for 'Go to Base' - Repetition 6\n",
      "Collecting data for 'Go to Base' - Repetition 7\n",
      "Collecting data for 'Go to Base' - Repetition 8\n",
      "Collecting data for 'Go to Base' - Repetition 9\n",
      "Collecting data for 'Go to Base' - Repetition 10\n",
      "Collecting data for 'Go to Base' - Repetition 11\n",
      "Collecting data for 'Go to Base' - Repetition 12\n",
      "Collecting data for 'Go to Base' - Repetition 13\n",
      "Collecting data for 'Go to Base' - Repetition 14\n",
      "Collecting data for 'Go to Base' - Repetition 15\n",
      "Collecting data for 'Go to Base' - Repetition 16\n",
      "Collecting data for 'Go to Base' - Repetition 17\n",
      "Collecting data for 'Go to Base' - Repetition 18\n",
      "Collecting data for 'Go to Base' - Repetition 19\n",
      "Collecting data for 'Go to Base' - Repetition 20\n",
      "Collecting data for 'Go to Base' - Repetition 21\n",
      "Collecting data for 'Go to Base' - Repetition 22\n",
      "Collecting data for 'Go to Base' - Repetition 23\n",
      "Collecting data for 'Go to Base' - Repetition 24\n",
      "Collecting data for 'Go to Base' - Repetition 25\n",
      "Collecting data for 'Go to Base' - Repetition 26\n",
      "Collecting data for 'Go to Base' - Repetition 27\n",
      "Collecting data for 'Go to Base' - Repetition 28\n",
      "Collecting data for 'Go to Base' - Repetition 29\n",
      "Collecting data for 'Go to Base' - Repetition 30\n",
      "Collecting data for 'Follow' - Repetition 1\n",
      "Collecting data for 'Follow' - Repetition 2\n",
      "Collecting data for 'Follow' - Repetition 3\n",
      "Collecting data for 'Follow' - Repetition 4\n",
      "Collecting data for 'Follow' - Repetition 5\n",
      "Collecting data for 'Follow' - Repetition 6\n",
      "Collecting data for 'Follow' - Repetition 7\n",
      "Collecting data for 'Follow' - Repetition 8\n",
      "Collecting data for 'Follow' - Repetition 9\n",
      "Collecting data for 'Follow' - Repetition 10\n",
      "Collecting data for 'Follow' - Repetition 11\n",
      "Collecting data for 'Follow' - Repetition 12\n",
      "Collecting data for 'Follow' - Repetition 13\n",
      "Collecting data for 'Follow' - Repetition 14\n",
      "Collecting data for 'Follow' - Repetition 15\n",
      "Collecting data for 'Follow' - Repetition 16\n",
      "Collecting data for 'Follow' - Repetition 17\n",
      "Collecting data for 'Follow' - Repetition 18\n",
      "Collecting data for 'Follow' - Repetition 19\n",
      "Collecting data for 'Follow' - Repetition 20\n",
      "Collecting data for 'Follow' - Repetition 21\n",
      "Collecting data for 'Follow' - Repetition 22\n",
      "Collecting data for 'Follow' - Repetition 23\n",
      "Collecting data for 'Follow' - Repetition 24\n",
      "Collecting data for 'Follow' - Repetition 25\n",
      "Collecting data for 'Follow' - Repetition 26\n",
      "Collecting data for 'Follow' - Repetition 27\n",
      "Collecting data for 'Follow' - Repetition 28\n",
      "Collecting data for 'Follow' - Repetition 29\n",
      "Collecting data for 'Follow' - Repetition 30\n",
      "Collecting data for 'Stop' - Repetition 1\n",
      "Collecting data for 'Stop' - Repetition 2\n",
      "Collecting data for 'Stop' - Repetition 3\n",
      "Collecting data for 'Stop' - Repetition 4\n",
      "Collecting data for 'Stop' - Repetition 5\n",
      "Collecting data for 'Stop' - Repetition 6\n",
      "Collecting data for 'Stop' - Repetition 7\n",
      "Collecting data for 'Stop' - Repetition 8\n",
      "Collecting data for 'Stop' - Repetition 9\n",
      "Collecting data for 'Stop' - Repetition 10\n",
      "Collecting data for 'Stop' - Repetition 11\n",
      "Collecting data for 'Stop' - Repetition 12\n",
      "Collecting data for 'Stop' - Repetition 13\n",
      "Collecting data for 'Stop' - Repetition 14\n",
      "Collecting data for 'Stop' - Repetition 15\n",
      "Collecting data for 'Stop' - Repetition 16\n",
      "Collecting data for 'Stop' - Repetition 17\n",
      "Collecting data for 'Stop' - Repetition 18\n",
      "Collecting data for 'Stop' - Repetition 19\n",
      "Collecting data for 'Stop' - Repetition 20\n",
      "Collecting data for 'Stop' - Repetition 21\n",
      "Collecting data for 'Stop' - Repetition 22\n",
      "Collecting data for 'Stop' - Repetition 23\n",
      "Collecting data for 'Stop' - Repetition 24\n",
      "Collecting data for 'Stop' - Repetition 25\n",
      "Collecting data for 'Stop' - Repetition 26\n",
      "Collecting data for 'Stop' - Repetition 27\n",
      "Collecting data for 'Stop' - Repetition 28\n",
      "Collecting data for 'Stop' - Repetition 29\n",
      "Collecting data for 'Stop' - Repetition 30\n",
      "Collecting data for 'Turn Left' - Repetition 1\n",
      "Collecting data for 'Turn Left' - Repetition 2\n",
      "Collecting data for 'Turn Left' - Repetition 3\n",
      "Collecting data for 'Turn Left' - Repetition 4\n",
      "Collecting data for 'Turn Left' - Repetition 5\n",
      "Collecting data for 'Turn Left' - Repetition 6\n",
      "Collecting data for 'Turn Left' - Repetition 7\n",
      "Collecting data for 'Turn Left' - Repetition 8\n",
      "Collecting data for 'Turn Left' - Repetition 9\n",
      "Collecting data for 'Turn Left' - Repetition 10\n",
      "Collecting data for 'Turn Left' - Repetition 11\n",
      "Collecting data for 'Turn Left' - Repetition 12\n",
      "Collecting data for 'Turn Left' - Repetition 13\n",
      "Collecting data for 'Turn Left' - Repetition 14\n",
      "Collecting data for 'Turn Left' - Repetition 15\n",
      "Collecting data for 'Turn Left' - Repetition 16\n",
      "Collecting data for 'Turn Left' - Repetition 17\n",
      "Collecting data for 'Turn Left' - Repetition 18\n",
      "Collecting data for 'Turn Left' - Repetition 19\n",
      "Collecting data for 'Turn Left' - Repetition 20\n",
      "Collecting data for 'Turn Left' - Repetition 21\n",
      "Collecting data for 'Turn Left' - Repetition 22\n",
      "Collecting data for 'Turn Left' - Repetition 23\n",
      "Collecting data for 'Turn Left' - Repetition 24\n",
      "Collecting data for 'Turn Left' - Repetition 25\n",
      "Collecting data for 'Turn Left' - Repetition 26\n",
      "Collecting data for 'Turn Left' - Repetition 27\n",
      "Collecting data for 'Turn Left' - Repetition 28\n",
      "Collecting data for 'Turn Left' - Repetition 29\n",
      "Collecting data for 'Turn Left' - Repetition 30\n",
      "Collecting data for 'Turn Right' - Repetition 1\n",
      "Collecting data for 'Turn Right' - Repetition 2\n",
      "Collecting data for 'Turn Right' - Repetition 3\n",
      "Collecting data for 'Turn Right' - Repetition 4\n",
      "Collecting data for 'Turn Right' - Repetition 5\n",
      "Collecting data for 'Turn Right' - Repetition 6\n",
      "Collecting data for 'Turn Right' - Repetition 7\n",
      "Collecting data for 'Turn Right' - Repetition 8\n",
      "Data collection paused. Press 'p' to resume.\n",
      "Resuming data collection.\n",
      "Collecting data for 'Turn Right' - Repetition 9\n",
      "Collecting data for 'Turn Right' - Repetition 10\n",
      "Collecting data for 'Turn Right' - Repetition 11\n",
      "Collecting data for 'Turn Right' - Repetition 12\n",
      "Collecting data for 'Turn Right' - Repetition 13\n",
      "Collecting data for 'Turn Right' - Repetition 14\n",
      "Collecting data for 'Turn Right' - Repetition 15\n",
      "Collecting data for 'Turn Right' - Repetition 16\n",
      "Collecting data for 'Turn Right' - Repetition 17\n",
      "Collecting data for 'Turn Right' - Repetition 18\n",
      "Collecting data for 'Turn Right' - Repetition 19\n",
      "Collecting data for 'Turn Right' - Repetition 20\n",
      "Collecting data for 'Turn Right' - Repetition 21\n",
      "Collecting data for 'Turn Right' - Repetition 22\n",
      "Collecting data for 'Turn Right' - Repetition 23\n",
      "Collecting data for 'Turn Right' - Repetition 24\n",
      "Collecting data for 'Turn Right' - Repetition 25\n",
      "Collecting data for 'Turn Right' - Repetition 26\n",
      "Collecting data for 'Turn Right' - Repetition 27\n",
      "Collecting data for 'Turn Right' - Repetition 28\n",
      "Collecting data for 'Turn Right' - Repetition 29\n",
      "Collecting data for 'Turn Right' - Repetition 30\n",
      "Collecting data for 'Move Forward' - Repetition 1\n",
      "Collecting data for 'Move Forward' - Repetition 2\n",
      "Collecting data for 'Move Forward' - Repetition 3\n",
      "Collecting data for 'Move Forward' - Repetition 4\n",
      "Collecting data for 'Move Forward' - Repetition 5\n",
      "Collecting data for 'Move Forward' - Repetition 6\n",
      "Collecting data for 'Move Forward' - Repetition 7\n",
      "Collecting data for 'Move Forward' - Repetition 8\n",
      "Collecting data for 'Move Forward' - Repetition 9\n",
      "Collecting data for 'Move Forward' - Repetition 10\n",
      "Collecting data for 'Move Forward' - Repetition 11\n",
      "Collecting data for 'Move Forward' - Repetition 12\n",
      "Collecting data for 'Move Forward' - Repetition 13\n",
      "Collecting data for 'Move Forward' - Repetition 14\n",
      "Collecting data for 'Move Forward' - Repetition 15\n",
      "Collecting data for 'Move Forward' - Repetition 16\n",
      "Collecting data for 'Move Forward' - Repetition 17\n",
      "Collecting data for 'Move Forward' - Repetition 18\n",
      "Collecting data for 'Move Forward' - Repetition 19\n",
      "Collecting data for 'Move Forward' - Repetition 20\n",
      "Collecting data for 'Move Forward' - Repetition 21\n",
      "Collecting data for 'Move Forward' - Repetition 22\n",
      "Collecting data for 'Move Forward' - Repetition 23\n",
      "Collecting data for 'Move Forward' - Repetition 24\n",
      "Collecting data for 'Move Forward' - Repetition 25\n",
      "Collecting data for 'Move Forward' - Repetition 26\n",
      "Collecting data for 'Move Forward' - Repetition 27\n",
      "Collecting data for 'Move Forward' - Repetition 28\n",
      "Collecting data for 'Move Forward' - Repetition 29\n",
      "Collecting data for 'Move Forward' - Repetition 30\n",
      "Collecting data for 'Move Backward' - Repetition 1\n",
      "Data collection paused. Press 'p' to resume.\n",
      "Resuming data collection.\n",
      "Collecting data for 'Move Backward' - Repetition 2\n",
      "Collecting data for 'Move Backward' - Repetition 3\n",
      "Collecting data for 'Move Backward' - Repetition 4\n",
      "Collecting data for 'Move Backward' - Repetition 5\n",
      "Collecting data for 'Move Backward' - Repetition 6\n",
      "Collecting data for 'Move Backward' - Repetition 7\n",
      "Collecting data for 'Move Backward' - Repetition 8\n",
      "Collecting data for 'Move Backward' - Repetition 9\n",
      "Collecting data for 'Move Backward' - Repetition 10\n",
      "Collecting data for 'Move Backward' - Repetition 11\n",
      "Collecting data for 'Move Backward' - Repetition 12\n",
      "Collecting data for 'Move Backward' - Repetition 13\n",
      "Collecting data for 'Move Backward' - Repetition 14\n",
      "Collecting data for 'Move Backward' - Repetition 15\n",
      "Collecting data for 'Move Backward' - Repetition 16\n",
      "Collecting data for 'Move Backward' - Repetition 17\n",
      "Collecting data for 'Move Backward' - Repetition 18\n",
      "Collecting data for 'Move Backward' - Repetition 19\n",
      "Collecting data for 'Move Backward' - Repetition 20\n",
      "Collecting data for 'Move Backward' - Repetition 21\n",
      "Collecting data for 'Move Backward' - Repetition 22\n",
      "Collecting data for 'Move Backward' - Repetition 23\n",
      "Collecting data for 'Move Backward' - Repetition 24\n",
      "Collecting data for 'Move Backward' - Repetition 25\n",
      "Collecting data for 'Move Backward' - Repetition 26\n",
      "Collecting data for 'Move Backward' - Repetition 27\n",
      "Collecting data for 'Move Backward' - Repetition 28\n",
      "Collecting data for 'Move Backward' - Repetition 29\n",
      "Collecting data for 'Move Backward' - Repetition 30\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Create the dataset directory if it doesn't exist\n",
    "dataset_dir = \"Dataset\"\n",
    "if not os.path.exists(dataset_dir):\n",
    "    os.makedirs(dataset_dir)\n",
    "\n",
    "# Map action labels to folder names\n",
    "actions = {\n",
    "    0: \"Lock Person\",\n",
    "    1: \"Unlock Person\",\n",
    "    2: \"Go to Base\",\n",
    "    3: \"Follow\",\n",
    "    4: \"Stop\",\n",
    "    5: \"Turn Left\",\n",
    "    6: \"Turn Right\",\n",
    "    7: \"Move Forward\",\n",
    "    8: \"Move Backward\"\n",
    "}\n",
    "\n",
    "# Set up MediaPipe Holistic\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Webcam parameters\n",
    "#width, height = 640, 480\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "#cap.set(3, width)\n",
    "#cap.set(4, height)\n",
    "\n",
    "# Number of frames to collect for each action\n",
    "num_frames_per_action = 60\n",
    "\n",
    "# Number of repetitions for each action\n",
    "num_repetitions = 30\n",
    "\n",
    "# Data augmentation parameters\n",
    "flip_prob = 0.5  # Probability of flipping the data\n",
    "rotation_range = 20  # Range of rotation angle in degrees\n",
    "scale_range = (0.8, 1.2)  # Range of scaling factors\n",
    "noise_std = 0.05  # Standard deviation of Gaussian noise\n",
    "\n",
    "for action_idx, action_label in actions.items():\n",
    "    # Create the action's directory if it doesn't exist\n",
    "    action_dir = os.path.join(dataset_dir, action_label)\n",
    "    if not os.path.exists(action_dir):\n",
    "        os.makedirs(action_dir)\n",
    "\n",
    "    for repetition in range(num_repetitions):\n",
    "        # Inform the start of data collection for each frame\n",
    "        print(f\"Collecting data for '{action_label}' - Repetition {repetition + 1}\")\n",
    "\n",
    "        # Pause for 5 seconds before starting data collection\n",
    "        time.sleep(5)\n",
    "\n",
    "        action_data = []\n",
    "\n",
    "        for frame_num in range(num_frames_per_action):\n",
    "            # Read frame from webcam\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            # Rescale the frame by a factor of 0.5\n",
    "            frame = cv2.resize(frame, None, fx=0.5, fy=0.5)\n",
    "\n",
    "            # Convert frame to RGB\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Process the frame with MediaPipe Holistic\n",
    "            with mp_holistic.Holistic(static_image_mode=False, model_complexity=2, min_detection_confidence=0.5,\n",
    "                                      min_tracking_confidence=0.5) as holistic:\n",
    "                # Perform landmark detection\n",
    "                results = holistic.process(frame_rgb)\n",
    "\n",
    "                # Check if landmarks are detected\n",
    "                if results.pose_landmarks and results.left_hand_landmarks and results.right_hand_landmarks:\n",
    "                    landmark_coords = []\n",
    "\n",
    "                    # Extract relevant landmarks (arms and nose)\n",
    "                    arm_landmarks_indices = [\n",
    "                        mp_holistic.PoseLandmark.LEFT_WRIST, mp_holistic.PoseLandmark.RIGHT_WRIST,\n",
    "                        mp_holistic.PoseLandmark.LEFT_ELBOW, mp_holistic.PoseLandmark.RIGHT_ELBOW,\n",
    "                        mp_holistic.PoseLandmark.LEFT_SHOULDER, mp_holistic.PoseLandmark.RIGHT_SHOULDER\n",
    "                    ]\n",
    "                    nose_landmark_index = mp_holistic.PoseLandmark.NOSE.value\n",
    "\n",
    "                    # Extract arm landmarks\n",
    "                    for landmark_idx in arm_landmarks_indices:\n",
    "                        landmark = results.pose_landmarks.landmark[landmark_idx]\n",
    "                        landmark_coords.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "                    # Extract nose landmark\n",
    "                    nose_landmark = results.pose_landmarks.landmark[nose_landmark_index]\n",
    "                    landmark_coords.append([nose_landmark.x, nose_landmark.y, nose_landmark.z])\n",
    "\n",
    "                    # Extract left hand landmarks\n",
    "                    for landmark in results.left_hand_landmarks.landmark:\n",
    "                        landmark_coords.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "                    # Extract right hand landmarks\n",
    "                    for landmark in results.right_hand_landmarks.landmark:\n",
    "                        landmark_coords.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "                    # Append the landmark coordinates to the action's data\n",
    "                    action_data.append(landmark_coords)\n",
    "\n",
    "                    # Display landmarks and connections on the frame\n",
    "                    mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "                    mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "                    mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "\n",
    "                    # Draw action label, frame number, and repetition number on the frame\n",
    "                    cv2.putText(frame, f\"Action: {action_label}\", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2,\n",
    "                                cv2.LINE_AA)\n",
    "                    cv2.putText(frame, f\"Frame: {frame_num + 1}\", (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2,\n",
    "                                cv2.LINE_AA)\n",
    "                    cv2.putText(frame, f\"Repetition: {repetition + 1}\", (20, 120), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                                (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                    # Show the frame\n",
    "                    cv2.imshow('Data Collection', frame)\n",
    "\n",
    "            # Exit loop if 'q' is pressed\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('p'):  # Pause data collection when 'p' is pressed\n",
    "                print(\"Data collection paused. Press 'p' to resume.\")\n",
    "                while True:\n",
    "                    key = cv2.waitKey(1) & 0xFF\n",
    "                    if key == ord('p'):  # Resume data collection when 'p' is pressed again\n",
    "                        print(\"Resuming data collection.\")\n",
    "                        break\n",
    "\n",
    "        # Convert the action's data to a numpy array\n",
    "        action_data = np.array(action_data)\n",
    "\n",
    "        # Save the action's data as a numpy array\n",
    "        np.save(os.path.join(action_dir, f\"{repetition}.npy\"), action_data)\n",
    "\n",
    "        # Apply data augmentation to generate additional data\n",
    "        augmented_data = []\n",
    "\n",
    "        for original_frame_data in action_data:\n",
    "            augmented_frame_data = np.copy(original_frame_data)\n",
    "\n",
    "            if np.random.uniform(0, 1) < flip_prob:\n",
    "                # Flip horizontally\n",
    "                augmented_frame_data[:, 0] = 1 - augmented_frame_data[:, 0]  # Flip x-coordinates\n",
    "\n",
    "            # Rotate the frame\n",
    "            angle = np.random.uniform(-rotation_range, rotation_range)\n",
    "            rows, cols, _ = frame.shape\n",
    "            M = cv2.getRotationMatrix2D((cols / 2, rows / 2), angle, 1)\n",
    "            augmented_frame_data[:, :2] = cv2.transform(augmented_frame_data[:, :2].reshape(1, -1, 2), M).reshape(-1, 2)\n",
    "\n",
    "            # Scale the frame\n",
    "            scale_factor = np.random.uniform(scale_range[0], scale_range[1])\n",
    "            augmented_frame_data[:, :2] *= scale_factor\n",
    "\n",
    "            # Add Gaussian noise to the frame\n",
    "            noise = np.random.normal(0, noise_std, augmented_frame_data[:, :2].shape)\n",
    "            augmented_frame_data[:, :2] += noise\n",
    "\n",
    "            augmented_data.append(augmented_frame_data)\n",
    "\n",
    "        augmented_data = np.array(augmented_data)\n",
    "\n",
    "        # Save the augmented data as an additional repetition\n",
    "        np.save(os.path.join(action_dir, f\"{repetition + num_repetitions}.npy\"), augmented_data)\n",
    "\n",
    "    # Exit loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      " 38/308 [==>...........................] - ETA: 43s - loss: 2.1809 - accuracy: 0.1768"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     76\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 77\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     79\u001b[0m inference_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1683\u001b[0m ):\n\u001b[0;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_no_variable_creation_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    141\u001b[0m   (concrete_function,\n\u001b[0;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1760\u001b[0m     args,\n\u001b[0;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1762\u001b[0m     executing_eagerly)\n\u001b[0;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "dataset_dir = \"Dataset\"\n",
    "\n",
    "# Get the list of actions from the dataset directory\n",
    "actions = sorted(os.listdir(dataset_dir))\n",
    "\n",
    "# Set the number of repetitions per action\n",
    "num_repetitions = 30\n",
    "\n",
    "# Create empty lists to store the data and labels\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Iterate over each action\n",
    "for action_label in actions:\n",
    "    action_dir = os.path.join(dataset_dir, action_label)\n",
    "    # Iterate over each repetition of the action\n",
    "    for repetition in range(num_repetitions):\n",
    "        # Load the data for the current repetition\n",
    "        action_data = np.load(os.path.join(action_dir, f\"{repetition}.npy\"))\n",
    "        data.extend(action_data)\n",
    "        labels.extend([action_label] * len(action_data))\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the data\n",
    "X_train = X_train / np.max(X_train)\n",
    "X_test = X_test / np.max(X_test)\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = len(actions)\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=128, return_sequences=True, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(LSTM(256, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(128, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping criteria\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "history = model.fit(X_train, y_train, epochs=2000, batch_size=32, validation_split=0.2, callbacks=[early_stop])\n",
    "end_time = time.time()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Save the trained LSTM model\n",
    "model.save('lstm_model.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot accuracy and loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate confusion matrix\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_true_labels = np.argmax(y_test, axis=1)\n",
    "cm = confusion_matrix(y_true_labels, y_pred_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", fmt=\"d\", xticklabels=actions, yticklabels=actions)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "# Print inference time\n",
    "print(f\"Inference Time: {inference_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSTM model in the provided code takes a sequence of 60 frames, where each frame represents the coordinates of various landmarks detected from the pose, left hand, and right hand. The shape of the input data is (60, 49, 3), where:\n",
    "\n",
    "- 60 represents the number of frames in a sequence.\n",
    "- 49 represents the number of landmarks and joints considered in each frame.\n",
    "- 3 represents the x, y, and z coordinates of each landmark.\n",
    "\n",
    "The model outputs a probability distribution over the different action classes. The shape of the output is (1, num_classes), where num_classes is the total number of action classes. The probability distribution is obtained using the softmax activation function, where each element in the output represents the probability of the corresponding action class.\n",
    "\n",
    "During prediction, the frames are passed through the model in batches. The model predicts the action class for each frame in the sequence. The predicted action class is determined by finding the index with the highest probability in the output. The final recognized action is the one with the highest probability among the predicted action classes.\n",
    "\n",
    "In summary, the input shape of the LSTM model is (60, 49, 3), and the output shape is (1, num_classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test in Real Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 290ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained LSTM model\n",
    "model = load_model('lstm_model.h5')\n",
    "\n",
    "# Map action labels to folder names\n",
    "actions = {\n",
    "    0: \"Lock Person\",\n",
    "    1: \"Unlock Person\",\n",
    "    2: \"Go to Base\",\n",
    "    3: \"Follow\",\n",
    "    4: \"Stop\",\n",
    "    5: \"Turn Left\",\n",
    "    6: \"Turn Right\",\n",
    "    7: \"Move Forward\",\n",
    "    8: \"Move Backward\"\n",
    "}\n",
    "\n",
    "# Set up MediaPipe Holistic\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Webcam parameters\n",
    "#width, height = 640, 480\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "#cap.set(3, width)\n",
    "#cap.set(4, height)\n",
    "\n",
    "while True:\n",
    "    # Read frame from webcam\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Rescale the frame by a factor of 0.5\n",
    "    #frame = cv2.resize(frame, None, fx=0.5, fy=0.5)\n",
    "\n",
    "    # Convert frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with MediaPipe Holistic\n",
    "    with mp_holistic.Holistic(static_image_mode=False, model_complexity=2, min_detection_confidence=0.5,\n",
    "                              min_tracking_confidence=0.5) as holistic:\n",
    "        # Perform landmark detection\n",
    "        results = holistic.process(frame_rgb)\n",
    "\n",
    "        # Check if landmarks are detected\n",
    "        if results.pose_landmarks and results.left_hand_landmarks and results.right_hand_landmarks:\n",
    "            landmark_coords = []\n",
    "\n",
    "            # Extract relevant landmarks (arms and nose)\n",
    "            arm_landmarks_indices = [\n",
    "                mp_holistic.PoseLandmark.LEFT_WRIST, mp_holistic.PoseLandmark.RIGHT_WRIST,\n",
    "                mp_holistic.PoseLandmark.LEFT_ELBOW, mp_holistic.PoseLandmark.RIGHT_ELBOW,\n",
    "                mp_holistic.PoseLandmark.LEFT_SHOULDER, mp_holistic.PoseLandmark.RIGHT_SHOULDER\n",
    "            ]\n",
    "            nose_landmark_index = mp_holistic.PoseLandmark.NOSE.value\n",
    "\n",
    "            # Extract arm landmarks\n",
    "            for landmark_idx in arm_landmarks_indices:\n",
    "                landmark = results.pose_landmarks.landmark[landmark_idx]\n",
    "                landmark_coords.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            # Extract nose landmark\n",
    "            nose_landmark = results.pose_landmarks.landmark[nose_landmark_index]\n",
    "            landmark_coords.append([nose_landmark.x, nose_landmark.y, nose_landmark.z])\n",
    "\n",
    "            # Extract left hand landmarks\n",
    "            for landmark in results.left_hand_landmarks.landmark:\n",
    "                landmark_coords.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            # Extract right hand landmarks\n",
    "            for landmark in results.right_hand_landmarks.landmark:\n",
    "                landmark_coords.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            # Convert the landmark coordinates to a numpy array\n",
    "            landmark_coords = np.array([landmark_coords])\n",
    "\n",
    "            # Make a prediction using the LSTM model\n",
    "            prediction = model.predict(landmark_coords)\n",
    "            action_index = np.argmax(prediction, axis=1)\n",
    "            # Get the scalar value from the NumPy array\n",
    "            action_index_scalar = action_index[0]  \n",
    "            action_label = actions[action_index_scalar]\n",
    "            action_prob = prediction[0][action_index_scalar]\n",
    "\n",
    "            # Draw action label and probability on the frame\n",
    "            cv2.putText(frame, f\"Action: {action_label}\", (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2,\n",
    "                        cv2.LINE_AA)\n",
    "            cv2.putText(frame, f\"Probability: {action_prob:.2f}\", (20, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0),\n",
    "                        2, cv2.LINE_AA)\n",
    "\n",
    "    # Show the frame\n",
    "    cv2.imshow('Gesture Recognition', frame)\n",
    "\n",
    "    # Exit loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 8.15259337e-01,  7.67898023e-01, -6.31418407e-01],\n",
       "        [ 4.13174868e-01,  8.34269404e-01, -1.20221210e+00],\n",
       "        [ 7.18261540e-01,  1.02987123e+00, -1.32918081e-04],\n",
       "        [ 1.03932500e-01,  1.01202607e+00, -6.33048296e-01],\n",
       "        [ 5.65004349e-01,  7.20924973e-01,  2.40592554e-01],\n",
       "        [ 2.17606500e-01,  7.25757658e-01, -1.41354427e-01],\n",
       "        [ 4.72011626e-01,  4.60155666e-01, -4.28829849e-01],\n",
       "        [ 8.02263081e-01,  7.38495827e-01, -6.32011421e-08],\n",
       "        [ 7.87428677e-01,  6.85195088e-01, -1.01244003e-02],\n",
       "        [ 7.95692444e-01,  6.25590622e-01, -1.14846285e-02],\n",
       "        [ 8.16576838e-01,  5.88951111e-01, -1.41014270e-02],\n",
       "        [ 8.32780182e-01,  5.60314059e-01, -1.57610383e-02],\n",
       "        [ 8.44454944e-01,  6.03753209e-01,  9.34872124e-03],\n",
       "        [ 8.64326954e-01,  5.69800556e-01,  2.90703471e-03],\n",
       "        [ 8.74415338e-01,  5.48720539e-01, -4.63646790e-03],\n",
       "        [ 8.82957757e-01,  5.30719757e-01, -1.06496951e-02],\n",
       "        [ 8.63845527e-01,  6.23764098e-01,  6.41527399e-03],\n",
       "        [ 8.91045332e-01,  5.89982569e-01, -7.65991956e-03],\n",
       "        [ 8.81008208e-01,  5.95365465e-01, -1.70179885e-02],\n",
       "        [ 8.66981149e-01,  6.07871711e-01, -1.92102678e-02],\n",
       "        [ 8.77354205e-01,  6.47190511e-01,  9.86663625e-04],\n",
       "        [ 9.03244019e-01,  6.19935930e-01, -1.20308883e-02],\n",
       "        [ 8.90638411e-01,  6.26477182e-01, -1.42081873e-02],\n",
       "        [ 8.77366602e-01,  6.40113711e-01, -1.16826184e-02],\n",
       "        [ 8.87355387e-01,  6.76345944e-01, -5.98602509e-03],\n",
       "        [ 9.09559429e-01,  6.53760254e-01, -1.39351664e-02],\n",
       "        [ 8.99390042e-01,  6.54348612e-01, -1.32871037e-02],\n",
       "        [ 8.86963189e-01,  6.64758682e-01, -1.05179911e-02],\n",
       "        [ 4.16278481e-01,  8.10395241e-01,  1.97931001e-07],\n",
       "        [ 4.46890473e-01,  7.37078190e-01,  2.29998175e-02],\n",
       "        [ 4.93792862e-01,  6.90823436e-01,  2.06287876e-02],\n",
       "        [ 5.42528987e-01,  6.69849455e-01,  1.15304152e-02],\n",
       "        [ 5.82846224e-01,  6.59938633e-01,  2.21293210e-03],\n",
       "        [ 5.05166829e-01,  6.47337794e-01, -5.74181834e-03],\n",
       "        [ 5.87086797e-01,  6.44382179e-01, -1.96249802e-02],\n",
       "        [ 6.20473206e-01,  6.71943426e-01, -2.38787383e-02],\n",
       "        [ 6.36804104e-01,  6.98002279e-01, -2.39072610e-02],\n",
       "        [ 5.05463362e-01,  6.70368433e-01, -2.64546312e-02],\n",
       "        [ 5.98686814e-01,  6.81406438e-01, -3.89234871e-02],\n",
       "        [ 6.26221538e-01,  7.18291759e-01, -3.84794809e-02],\n",
       "        [ 6.31431699e-01,  7.45808184e-01, -3.67296934e-02],\n",
       "        [ 5.05524695e-01,  7.04666197e-01, -4.49782088e-02],\n",
       "        [ 5.95972598e-01,  7.17964292e-01, -5.00592403e-02],\n",
       "        [ 6.22567058e-01,  7.52956152e-01, -4.28177081e-02],\n",
       "        [ 6.26598001e-01,  7.78081894e-01, -3.79152112e-02],\n",
       "        [ 5.06106496e-01,  7.44880259e-01, -6.19240142e-02],\n",
       "        [ 5.77990592e-01,  7.51505613e-01, -6.11574911e-02],\n",
       "        [ 6.04577184e-01,  7.74562120e-01, -5.31926043e-02],\n",
       "        [ 6.14262402e-01,  7.93167353e-01, -4.71637882e-02]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "landmark_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(landmark_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9241806e-09, 1.9505488e-03, 6.9236339e-05, 1.0724920e-14,\n",
       "        4.7798853e-12, 1.9300388e-21, 3.9995871e-10, 1.8334742e-15,\n",
       "        9.9798030e-01]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_index_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Move Backward'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9979803"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inference_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 162\u001b[0m\n\u001b[0;32m    159\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFPS: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfps\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, fps_pos, cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.8\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Draw inference time on the frame\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minference_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, inference_pos, cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.8\u001b[39m,\n\u001b[0;32m    163\u001b[0m             (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Draw probability of recognized action on the frame\u001b[39;00m\n\u001b[0;32m    166\u001b[0m cv2\u001b[38;5;241m.\u001b[39mputText(frame, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProbability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction_prob[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, probability_pos, cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.8\u001b[39m,\n\u001b[0;32m    167\u001b[0m             (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'inference_time' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tensorflow.keras.models import load_model\n",
    "import time\n",
    "\n",
    "# Load the trained LSTM model\n",
    "model = load_model('lstm_model.h5')\n",
    "\n",
    "# Map action labels to folder names\n",
    "actions = {\n",
    "    0: \"Lock Person\",\n",
    "    1: \"Unlock Person\",\n",
    "    2: \"Go to Base\",\n",
    "    3: \"Follow\",\n",
    "    4: \"Stop\",\n",
    "    5: \"Turn Left\",\n",
    "    6: \"Turn Right\",\n",
    "    7: \"Move Forward\",\n",
    "    8: \"Move Backward\"\n",
    "}\n",
    "\n",
    "unknown_action_label = \"Unknown Action\"\n",
    "\n",
    "# Set up MediaPipe Holistic\n",
    "mp_holistic = mp.solutions.holistic\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Webcam parameters\n",
    "# width, height = 640, 480\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "# cap.set(3, width)\n",
    "# cap.set(4, height)\n",
    "\n",
    "# Variables for collecting frames\n",
    "frame_count = 0\n",
    "frames = []\n",
    "\n",
    "# Variables for action recognition\n",
    "recognized_action = None\n",
    "start_time = 0\n",
    "\n",
    "# Variables for FPS calculation\n",
    "prev_time = 0\n",
    "\n",
    "# Define the positions for displaying text\n",
    "fps_pos = (20, 40)\n",
    "inference_pos = (20, 70)\n",
    "probability_pos = (20, 100)\n",
    "\n",
    "while True:\n",
    "    # Read frame from webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Rescale the frame by a factor of 0.5\n",
    "    # frame = cv2.resize(frame, None, fx=0.5, fy=0.5)\n",
    "\n",
    "    # Flip the frame horizontally\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Convert frame to RGB\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame with MediaPipe Holistic\n",
    "    with mp_holistic.Holistic(static_image_mode=False, model_complexity=2, min_detection_confidence=0.5,\n",
    "                              min_tracking_confidence=0.5) as holistic:\n",
    "        # Perform landmark detection\n",
    "        results = holistic.process(frame_rgb)\n",
    "\n",
    "        # Check if landmarks are detected\n",
    "        if results.pose_landmarks and results.left_hand_landmarks and results.right_hand_landmarks:\n",
    "            landmark_coords = []\n",
    "\n",
    "            # Extract relevant landmarks (arms and nose)\n",
    "            arm_landmarks_indices = [\n",
    "                mp_holistic.PoseLandmark.LEFT_WRIST, mp_holistic.PoseLandmark.RIGHT_WRIST,\n",
    "                mp_holistic.PoseLandmark.LEFT_ELBOW, mp_holistic.PoseLandmark.RIGHT_ELBOW,\n",
    "                mp_holistic.PoseLandmark.LEFT_SHOULDER, mp_holistic.PoseLandmark.RIGHT_SHOULDER\n",
    "            ]\n",
    "            nose_landmark_index = mp_holistic.PoseLandmark.NOSE.value\n",
    "\n",
    "            # Extract arm landmarks\n",
    "            for landmark_idx in arm_landmarks_indices:\n",
    "                landmark = results.pose_landmarks.landmark[landmark_idx]\n",
    "                landmark_coords.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            # Extract nose landmark\n",
    "            nose_landmark = results.pose_landmarks.landmark[nose_landmark_index]\n",
    "            landmark_coords.append([nose_landmark.x, nose_landmark.y, nose_landmark.z])\n",
    "\n",
    "            # Extract left hand landmarks\n",
    "            for landmark in results.left_hand_landmarks.landmark:\n",
    "                landmark_coords.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            # Extract right hand landmarks\n",
    "            for landmark in results.right_hand_landmarks.landmark:\n",
    "                landmark_coords.append([landmark.x, landmark.y, landmark.z])\n",
    "\n",
    "            # Collect frames\n",
    "            frames.append(landmark_coords)\n",
    "            frame_count += 1\n",
    "            \n",
    "            # Convert the landmark coordinates to a numpy array\n",
    "            landmark_coords = np.array([landmark_coords])\n",
    "\n",
    "            # Make a prediction using the LSTM model\n",
    "            prediction = model.predict(landmark_coords)\n",
    "            action_index = np.argmax(prediction, axis=1)\n",
    "            # Get the scalar value from the NumPy array\n",
    "            action_index_scalar = action_index[0]  \n",
    "            action_label = actions[action_index_scalar]\n",
    "            action_prob = prediction[0][action_index_scalar]\n",
    "            \n",
    "            # When 60 frames are collected, make a prediction\n",
    "            if frame_count == 60:\n",
    "\n",
    "                # Convert the frames to a numpy array\n",
    "                frames = np.array(frames)\n",
    "\n",
    "                # Normalize the data\n",
    "                frames = frames / np.max(frames)\n",
    "\n",
    "                # Reshape the frames array\n",
    "                # frames = frames.reshape(60, 49, 3)\n",
    "\n",
    "                # Perform prediction\n",
    "                start_time = time.time()\n",
    "                prediction = model.predict(frames)\n",
    "                end_time = time.time()\n",
    "                inference_time = end_time - start_time\n",
    "\n",
    "                # Get the predicted action\n",
    "                action_index = np.argmax(prediction)\n",
    "                action_prob = prediction[0][action_index]\n",
    "                action_prob_final = np.argmax(action_prob)\n",
    "\n",
    "                # Check if the predicted action index is within the action labels dictionary\n",
    "                if action_index in actions:\n",
    "                    action_label = actions[action_index]\n",
    "                else:\n",
    "                    action_label = unknown_action_label\n",
    "\n",
    "                # Check if a new action has been recognized\n",
    "                if recognized_action != action_label:\n",
    "                    recognized_action = action_label\n",
    "\n",
    "                # Reset variables for collecting frames\n",
    "                frame_count = 0\n",
    "                frames = []\n",
    "\n",
    "    # Calculate and display FPS\n",
    "    current_time = time.time()\n",
    "    fps = 1 / (current_time - prev_time)\n",
    "    prev_time = current_time\n",
    "\n",
    "    # Draw FPS on the frame (top left corner)\n",
    "    cv2.putText(frame, f\"FPS: {fps:.2f}\", fps_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Draw inference time on the frame\n",
    "    cv2.putText(frame, f\"Inference Time: {inference_time:.2f}s\", inference_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.8,\n",
    "                (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Draw probability of recognized action on the frame\n",
    "    cv2.putText(frame, f\"Probability: {action_prob[0]:.2f}\", probability_pos, cv2.FONT_HERSHEY_SIMPLEX, 0.8,\n",
    "                (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    # Draw action labels on the frame (top right corner)\n",
    "    y = 40\n",
    "    for i, (action_id, action_name) in enumerate(actions.items()):\n",
    "        text_color = (0, 255, 0) if action_name == recognized_action else (255, 255, 255)\n",
    "        cv2.putText(frame, f\"{action_name}\", (width - 200, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, text_color, 2,\n",
    "                    cv2.LINE_AA)\n",
    "        y += 30\n",
    "\n",
    "    # Check for lock and unlock gestures\n",
    "    if recognized_action == \"Lock Person\":\n",
    "        cv2.putText(frame, \"Dynamic hand gesture recognition has started\", (20, height - 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    elif recognized_action == \"Unlock Person\":\n",
    "        cv2.putText(frame, \"Dynamic hand gesture recognition has stopped\", (20, height - 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Draw pose landmarks\n",
    "    mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                              connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2))\n",
    "\n",
    "    # Draw left hand landmarks\n",
    "    mp_drawing.draw_landmarks(frame, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              landmark_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                              connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2))\n",
    "\n",
    "    # Draw right hand landmarks\n",
    "    mp_drawing.draw_landmarks(frame, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2),\n",
    "                              connection_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2))\n",
    "\n",
    "    # Render the frame\n",
    "    cv2.imshow('Action Recognition', frame)\n",
    "\n",
    "    # Break the loop when 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and destroy all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "9.511863e-27",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_prob\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 9.511863e-27"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
